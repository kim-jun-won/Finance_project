{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "import sqlite3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import csv\n",
    "#git hub test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_naver_economy_news_urls_from_list(pages=5, max_urls=50):\n",
    "    base_url = \"https://news.naver.com/main/list.naver\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    all_links = set()\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        params = {\n",
    "            \"mode\": \"LSD\",\n",
    "            \"mid\": \"shm\",\n",
    "            \"sid1\": \"101\",  # 경제 뉴스 섹션\n",
    "            \"page\": str(page)\n",
    "        }\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        print(f\"{page}페이지 응답 코드: {response.status_code}\")\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"{page}페이지 요청 실패\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for a in soup.select(\"dt > a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and href.startswith(\"https://n.news.naver.com\"):\n",
    "                all_links.add(href)\n",
    "                if len(all_links) >= max_urls:\n",
    "                    print(f\"최대 {max_urls}개 링크 수집 완료, 수집 종료\")\n",
    "                    return list(all_links)\n",
    "\n",
    "    print(f\"총 {len(all_links)}개의 중복 없는 링크를 수집했습니다.\")\n",
    "    return list(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_title(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(\"❌ 뉴스 기사 크롤링 실패:\", response.status_code)\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    title_tag = soup.select_one(\"h2#title\") or soup.select_one(\"h3#articleTitle\")\n",
    "    if title_tag:\n",
    "        return title_tag.get_text(strip=True)\n",
    "\n",
    "    # <title> 태그 사용 (네이버 뉴스는 보통 '기사제목 - 네이버 뉴스' 형태)\n",
    "    if soup.title:\n",
    "        full_title = soup.title.get_text()\n",
    "        return full_title.replace(\" - 네이버 뉴스\", \"\").strip()\n",
    "    \n",
    "    print(\"❌ 뉴스 제목을 찾을 수 없음\")\n",
    "    return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_news_to_csv(filename, texts):\n",
    "    with open(filename, mode='w', newline='', encoding='utf-8-sig') as f:\n",
    "        writer = csv.writer(f)\n",
    "        # 헤더 작성\n",
    "        writer.writerow([\"text\", \"category\"])  \n",
    "        # category는 빈 칸으로 둠 (나중에 수동 입력)\n",
    "        for text in texts:\n",
    "            writer.writerow([text, \"\"])  \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 크롤링 & 단어 matching test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1페이지 응답 코드: 200\n",
      "최대 1개 링크 수집 완료, 수집 종료\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'news_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m title:\n\u001b[0;32m      7\u001b[0m         titles\u001b[38;5;241m.\u001b[39mappend(title)\n\u001b[1;32m----> 9\u001b[0m \u001b[43msave_news_to_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnews_data.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitles\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m, in \u001b[0;36msave_news_to_csv\u001b[1;34m(filename, texts)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_news_to_csv\u001b[39m(filename, texts):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8-sig\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      3\u001b[0m         writer \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mwriter(f)\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;66;03m# 헤더 작성\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'news_data.csv'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    news_urls = get_naver_economy_news_urls_from_list(1, max_urls=1)\n",
    "    titles = []\n",
    "    for url in news_urls:\n",
    "        title = get_news_title(url)\n",
    "        if title:\n",
    "            titles.append(title)\n",
    "    \n",
    "    save_news_to_csv(\"news_data.csv\", titles)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          국제       1.00      0.25      0.40         4\n",
      "          금융       0.83      0.38      0.53        13\n",
      "          기타       1.00      0.25      0.40         4\n",
      "         부동산       0.87      0.59      0.70        22\n",
      "          사회       0.00      0.00      0.00         3\n",
      "          산업       0.31      1.00      0.47        14\n",
      "          소비       1.00      0.50      0.67         4\n",
      "          정책       1.00      0.62      0.77        16\n",
      "\n",
      "    accuracy                           0.57        80\n",
      "   macro avg       0.75      0.45      0.49        80\n",
      "weighted avg       0.78      0.57      0.59        80\n",
      "\n",
      "분류 결과: 부동산\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 1. 데이터 불러오기 및 전처리\n",
    "df = pd.read_csv(\"news_data.csv\", encoding=\"euc-kr\")\n",
    "df[\"category\"] = df[\"category\"].str.strip()  # 공백 제거\n",
    "df = df[df[\"category\"].notnull()]  # NaN 제거\n",
    "\n",
    "X = df[\"text\"]\n",
    "y = df[\"category\"]\n",
    "\n",
    "# 2. TF-IDF 벡터화 (성능 개선을 위해 ngram 추가)\n",
    "vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "X_vec = vectorizer.fit_transform(X)\n",
    "\n",
    "# 3. 학습/테스트 분리\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. 로지스틱 회귀 모델 학습 (클래스 가중치 적용)\n",
    "model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 5. 성능 평가\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, zero_division=0))  # zero_division=0으로 경고 제거\n",
    "\n",
    "# 6. 새 기사 분류 함수\n",
    "def classify_article(text):\n",
    "    X_vec = vectorizer.transform([text])\n",
    "    return model.predict(X_vec)[0]\n",
    "\n",
    "# 예시 테스트\n",
    "print(\"분류 결과:\", classify_article(\"정부, 주택담보대출 규제 완화 검토…수요 회복 기대\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. \"삼성전자, 차세대 반도체 공정 개발 완료…양산 준비 돌입\" → 분류 결과: 산업\n",
      "2. \"정부, 주택담보대출 규제 완화 검토…수요 회복 기대\" → 분류 결과: 정책\n",
      "3. \"서울 아파트 전셋값 3개월 연속 상승…전세난 재현되나\" → 분류 결과: 부동산\n",
      "4. \"한은, 기준금리 동결…물가 안정 기조 유지\" → 분류 결과: 산업\n",
      "5. \"현대차, 미국에 전기차 공장 신설…총 10조 원 투자\" → 분류 결과: 산업\n",
      "6. \"중소기업청, 청년 창업 지원금 200억 원 추가 투입\" → 분류 결과: 산업\n",
      "7. \"카카오뱅크, 2분기 순이익 1200억…사상 최대 실적\" → 분류 결과: 산업\n",
      "8. \"롯데건설, 부산 해운대 대형 주거단지 수주\" → 분류 결과: 부동산\n",
      "9. \"미국 금리 인상 영향…원/달러 환율 1,350원 돌파\" → 분류 결과: 산업\n",
      "10. \"정부, 소비 회복 위해 추석 전 재난지원금 지급 추진\" → 분류 결과: 산업\n"
     ]
    }
   ],
   "source": [
    "# 예시 기사 10개\n",
    "test_articles = [\n",
    "    \"삼성전자, 차세대 반도체 공정 개발 완료…양산 준비 돌입\",\n",
    "    \"정부, 주택담보대출 규제 완화 검토…수요 회복 기대\",\n",
    "    \"서울 아파트 전셋값 3개월 연속 상승…전세난 재현되나\",\n",
    "    \"한은, 기준금리 동결…물가 안정 기조 유지\",\n",
    "    \"현대차, 미국에 전기차 공장 신설…총 10조 원 투자\",\n",
    "    \"중소기업청, 청년 창업 지원금 200억 원 추가 투입\",\n",
    "    \"카카오뱅크, 2분기 순이익 1200억…사상 최대 실적\",\n",
    "    \"롯데건설, 부산 해운대 대형 주거단지 수주\",\n",
    "    \"미국 금리 인상 영향…원/달러 환율 1,350원 돌파\",\n",
    "    \"정부, 소비 회복 위해 추석 전 재난지원금 지급 추진\"\n",
    "]\n",
    "\n",
    "# 분류 함수 (이전과 동일)\n",
    "def classify_article(text):\n",
    "    X_vec = vectorizer.transform([text])\n",
    "    return model.predict(X_vec)[0]\n",
    "\n",
    "# 결과 출력\n",
    "for i, article in enumerate(test_articles, 1):\n",
    "    category = classify_article(article)\n",
    "    print(f\"{i}. \\\"{article}\\\" → 분류 결과: {category}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
