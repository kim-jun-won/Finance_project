{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from konlpy.tag import Okt\n",
    "import sqlite3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from krwordrank.word import KRWordRank\n",
    "\n",
    "        \n",
    "# ğŸ”¹ ì œì™¸í•  ì˜ì–´ ê´€ì‚¬(articles) ëª©ë¡\n",
    "ARTICLES = {\"a\", \"an\", \"the\"}\n",
    "\n",
    "# ğŸ”¹ í•œê¸€ ì¡°ì‚¬ ëª©ë¡ (ë‹¨ì–´ ëì— ë¶™ëŠ” ê²½ìš° ì˜ë¯¸ ë¶„ì„ ë°©í•´ â†’ ì œê±° ëŒ€ìƒ)\n",
    "PARTICLES = {\n",
    "    \"ì€\",\n",
    "    \"ëŠ”\",\n",
    "    \"ì´\",\n",
    "    \"ê°€\",\n",
    "    \"ì„\",\n",
    "    \"ë¥¼\",\n",
    "    \"ì—\",\n",
    "    \"ì—ì„œ\",\n",
    "    \"ì™€\",\n",
    "    \"ê³¼\",\n",
    "    \"ë„\",\n",
    "    \"ì˜\",\n",
    "    \"í•œ\",\n",
    "    \"ë¡œ\",\n",
    "    \"ìœ¼ë¡œ\",\n",
    "    \"í•˜ê³ \",\n",
    "    \"ë°\",\n",
    "    \"ë“±\",\n",
    "    \"ê¹Œì§€\",\n",
    "    \"ë¶€í„°\",\n",
    "    \"ë§Œ\",\n",
    "    \"ë³´ë‹¤\",\n",
    "    \"ì²˜ëŸ¼\",\n",
    "    \"ê°™ì´\",\n",
    "    \"ê»˜ì„œ\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## URL Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_naver_economy_news_urls_from_list(pages=5, max_urls=50):\n",
    "    base_url = \"https://news.naver.com/main/list.naver\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    all_links = set()\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        params = {\n",
    "            \"mode\": \"LSD\",\n",
    "            \"mid\": \"shm\",\n",
    "            \"sid1\": \"101\",  # ê²½ì œ ë‰´ìŠ¤ ì„¹ì…˜\n",
    "            \"page\": str(page)\n",
    "        }\n",
    "        response = requests.get(base_url, headers=headers, params=params)\n",
    "        print(f\"{page}í˜ì´ì§€ ì‘ë‹µ ì½”ë“œ: {response.status_code}\")\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            print(f\"{page}í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨\")\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        for a in soup.select(\"dt > a\"):\n",
    "            href = a.get(\"href\")\n",
    "            if href and href.startswith(\"https://n.news.naver.com\"):\n",
    "                all_links.add(href)\n",
    "                if len(all_links) >= max_urls:\n",
    "                    print(f\"ìµœëŒ€ {max_urls}ê°œ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ, ìˆ˜ì§‘ ì¢…ë£Œ\")\n",
    "                    return list(all_links)\n",
    "\n",
    "    print(f\"ì´ {len(all_links)}ê°œì˜ ì¤‘ë³µ ì—†ëŠ” ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.\")\n",
    "    return list(all_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word crawling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_news_text(url):\n",
    "    \"\"\"\n",
    "    ë„¤ì´ë²„ ë‰´ìŠ¤ ê¸°ì‚¬ URLë¡œë¶€í„° ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ í¬ë¡¤ë§í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "\n",
    "    # ğŸ”¹ HTTP GET ìš”ì²­\n",
    "    response = requests.get(url, headers=headers)\n",
    "  \n",
    "    # ğŸ”¸ ì‘ë‹µ ìƒíƒœ ì½”ë“œ í™•ì¸\n",
    "    if response.status_code != 200:\n",
    "        print(\"âŒ ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹¤íŒ¨:\", response.status_code)\n",
    "        return \"\"\n",
    "\n",
    "    # ğŸ”¹ HTML íŒŒì‹±\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    # ğŸ”¸ ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ì´ ìˆëŠ” div ì„ íƒ (2023ë…„ ì´í›„ 'newsct_article' ì‚¬ìš©)\n",
    "    article_body = soup.select_one(\"div#newsct_article\")\n",
    "\n",
    "    if article_body:\n",
    "        # ğŸ”¹ ì¤„ë°”ê¿ˆ í¬í•¨í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "        return article_body.get_text(strip=True, separator=\"\\n\")\n",
    "    else:\n",
    "        print(\"âŒ ë‰´ìŠ¤ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_particles(word):\n",
    "    \"\"\"\n",
    "    í•œê¸€ ë‹¨ì–´ì—ì„œ ì¡°ì‚¬(ì¡°ì‚¬ ëª©ë¡ì— í¬í•¨ëœ ë‹¨ì–´)ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜\n",
    "    ì˜ˆ) \"ì „ì‚°ì¥ì• ë¡œ\" â†’ \"ì „ì‚°ì¥ì• \"\n",
    "    \"\"\"\n",
    "    # ì •ê·œí‘œí˜„ì‹ì„ í†µí•´ ë‹¨ì–´ ëì— ë¶™ì€ ì¡°ì‚¬ ì œê±°\n",
    "    pattern = r\"(\" + \"|\".join(PARTICLES) + r\")$\"\n",
    "    return re.sub(pattern, \"\", word)\n",
    "\n",
    "\n",
    "def extract_words_okt(text):\n",
    "    \"\"\"\n",
    "    ë³¸ë¬¸ì—ì„œ ëª…ì‚¬êµ¬ ì¶”ì¶œí•˜ê³  ì¡°ì‚¬ ì œê±°,\n",
    "    ì¤‘ë³µ ì—†ì´ ì²˜ë¦¬í•˜ì§€ ì•Šê³  ë“±ì¥ ìˆœì„œ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    okt = Okt()\n",
    "    nouns = okt.phrases(text)  # ëª…ì‚¬êµ¬ ì¶”ì¶œ\n",
    "\n",
    "    filtered = [remove_particles(n) for n in nouns if n.strip()]\n",
    "    return filtered  # ì¤‘ë³µ ì œê±° ì—†ì´ ê·¸ëŒ€ë¡œ ë°˜í™˜\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê¸°ì‚¬ ë‚´ í‚¤ì›Œë“œ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text_list, top_n=20):\n",
    "    \"\"\"\n",
    "    krwordrankë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "    \"\"\"\n",
    "    wordrank_extractor = KRWordRank(\n",
    "        min_count=2,     # ìµœì†Œ ë“±ì¥ íšŸìˆ˜\n",
    "        max_length=10,   # í‚¤ì›Œë“œ ìµœëŒ€ ê¸¸ì´\n",
    "        verbose=True     # ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "    )\n",
    "    \n",
    "    keywords, rank, graph = wordrank_extractor.extract(\n",
    "        text_list,\n",
    "        beta=0.85,       # í˜ì´ì§€ë­í¬ ê³„ìˆ˜\n",
    "        max_iter=10      # ë°˜ë³µ íšŸìˆ˜\n",
    "    )\n",
    "\n",
    "    sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_keywords[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Word List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_words_in_rows(words, words_per_row=10):\n",
    "    \"\"\"\n",
    "    ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ê°œìˆ˜(words_per_row)ë§Œí¼ í•œ ì¤„ì— ì¶œë ¥\n",
    "    \"\"\"\n",
    "    for i in range(0, len(words), words_per_row):\n",
    "        print(\", \".join(words[i : i + words_per_row]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DB Scanning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… DB ì¡°íšŒ í•¨ìˆ˜\n",
    "def find_description_from_db(conn, term_input):\n",
    "    cursor = conn.cursor()\n",
    "    query = \"SELECT * FROM dictionary WHERE term = ?\"\n",
    "    cursor.execute(query, (term_input,))\n",
    "    result = cursor.fetchone()\n",
    "    if result:\n",
    "        return result[3]  # description ì»¬ëŸ¼\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    # âœ… ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\n",
    "\n",
    "def extract_and_explain(url, db_path):\n",
    "    text = get_news_text(url)\n",
    "    \n",
    "    words_Konpy = extract_words_okt(text)\n",
    "    words_Konpy = list(dict.fromkeys(words_Konpy))  # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)\n",
    "\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    found = False  # âœ… ì´ˆê¸°í™”\n",
    "\n",
    "    print(\"ğŸ” ê¸°ì‚¬ì—ì„œ ë“±ì¥í•œ ê²½ì œ ìš©ì–´ ì„¤ëª… Konlpy:\\n\")\n",
    "    \n",
    "\n",
    "    for word in words_Konpy:\n",
    "        desc = find_description_from_db(conn, word)\n",
    "        if desc:\n",
    "            found = True\n",
    "            print(f\"ğŸ“Œ {word}: {desc}\\n\")       \n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    if not found:\n",
    "        print(\"ğŸ“ ê¸°ì‚¬ ë‚´ì—ì„œ ì„¤ëª… ê°€ëŠ¥í•œ ê²½ì œ ìš©ì–´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í¬ë¡¤ë§ & ë‹¨ì–´ matching test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1í˜ì´ì§€ ì‘ë‹µ ì½”ë“œ: 200\n",
      "ìµœëŒ€ 20ê°œ ë§í¬ ìˆ˜ì§‘ ì™„ë£Œ, ìˆ˜ì§‘ ì¢…ë£Œ\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/277/0005614737?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 178\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ì†ŒìŠ¤: 4.8866\n",
      "- ì™¸ì‹: 3.0009\n",
      "- ì œì¡°: 2.6365\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/015/0005150905?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 381\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ë””ì–´ìœ : 3.6273\n",
      "- ì¤‘êµ­: 3.1568\n",
      "- ì œê³µ: 3.0185\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/032/0003379274?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 163\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ì •ì±…: 3.119\n",
      "- â€˜AI: 2.6374\n",
      "- ì´ì¬ëª…: 2.572\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/001/0015476817?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 391\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ì¥ê´€: 5.4358\n",
      "- í›„ë³´ì: 4.1748\n",
      "- ì‚°ì—…: 4.0272\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/055/0001270667?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 202\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ì„œìš¸: 5.871\n",
      "- ì•„íŒŒíŠ¸: 3.7283\n",
      "- í‰ê· : 3.2055\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/009/0005516494?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 220\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ì˜ˆì‚°: 2.8395\n",
      "- ì˜¬í•´: 2.4602\n",
      "- ì¤€ê³µ: 2.4067\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/009/0005516498?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 271\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- LG: 8.1865\n",
      "- ëª¨ë“ˆëŸ¬: 5.0582\n",
      "- ì£¼íƒ: 3.9066\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/469/0000873118?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 231\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ì¼ë³¸: 6.7196\n",
      "- ë¯¸êµ­: 4.1611\n",
      "- ìˆë‹¤.: 4.1128\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/422/0000754376?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 133\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ì „ê¸°ì°¨: 5.6336\n",
      "- ì „ìš©: 4.3404\n",
      "- 100ë§ŒëŒ€ë¥¼: 2.9116\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/009/0005516495?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 344\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ìˆë‹¤.: 3.4763\n",
      "- í†µí•´: 2.993\n",
      "- ìì–‘ë™: 2.9764\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/015/0005150904?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 226\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ì•„íŒŒíŠ¸: 4.5362\n",
      "- ì„œìš¸: 4.0834\n",
      "- í‰ê· : 3.4626\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/081/0003553383?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 160\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ì¥ê´€ì´: 2.8971\n",
      "- ë…¼ì½©: 2.5012\n",
      "- ì¬ë°°: 2.2718\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/029/0002964592?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 303\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ì¶©ì „: 6.4483\n",
      "- ì „ê¸°ì°¨: 4.8859\n",
      "- ìš´ì˜í•˜ëŠ”: 3.4242\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/014/0005369640?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 192\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ë¬´ì£¼íƒ: 3.0768\n",
      "- â€˜ì‹ í˜¼Â·ì‹ ìƒì•„: 3.0138\n",
      "- â–³ë¶„ì–‘ì „í™˜í˜•: 2.9121\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/009/0005516493?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 231\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ì „ìš©: 4.4456\n",
      "- ë”ì›¨ì´ì‹œí‹°: 4.1916\n",
      "- ì§€í•˜: 2.4072\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/011/0004502713?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 374\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ì„œìš¸: 3.7834\n",
      "- ë§¤ë¬¼: 3.4037\n",
      "- ë§¤ìˆ˜: 3.2702\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/032/0003379273?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 351\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- íŠ¸ëŸ¼í”„: 5.3049\n",
      "- ê¸€ë¡œë²Œ: 5.086\n",
      "- ë¯¸êµ­: 4.2369\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/029/0002964593?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 325\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ë©”ëª¨ë¦¬: 5.472\n",
      "- ê¸°ìˆ : 4.3985\n",
      "- ëª¨ë‘: 3.6239\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/008/0005214245?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 243\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- êµ­ë¯¼ì˜: 4.0502\n",
      "- ì‚¬íšŒì•ˆì „ë§ì„: 3.5787\n",
      "- ë§í–ˆë‹¤.: 3.3413\n",
      "\n",
      "ğŸ“„ ê¸°ì‚¬ URL: https://n.news.naver.com/mnews/article/009/0005516497?sid=101\n",
      "scan vocabs ... \n",
      "num vocabs = 275\n",
      "done = 10\n",
      "ğŸ“Œ í‚¤ì›Œë“œ:\n",
      "- ë°°ë‹¬: 8.3385\n",
      "- ê°€ê²©ì„: 5.8137\n",
      "- ë§¤ì¥: 3.8616\n"
     ]
    }
   ],
   "source": [
    "# âœ… ì‹¤í–‰\n",
    "if __name__ == \"__main__\":\n",
    "    db_file = r\"C:\\Users\\wjdgh\\OneDrive\\ë°”íƒ• í™”ë©´\\2025-1\\2025ì¡¸ì—…ì‘í’ˆ\\Parsing&crawling\\dictionary_test.db\"\n",
    "    \n",
    "    # 1. ë‰´ìŠ¤ URL ìˆ˜ì§‘\n",
    "    news_urls = get_naver_economy_news_urls_from_list(1,max_urls=20)\n",
    "    \n",
    "    # 2. ê° ë‰´ìŠ¤ URLì— ëŒ€í•´ ì²˜ë¦¬\n",
    "for url in news_urls:\n",
    "    print(f\"\\nğŸ“„ ê¸°ì‚¬ URL: {url}\")\n",
    "    \n",
    "    text = get_news_text(url)\n",
    "    if not text.strip():\n",
    "        print(\"âŒ ê¸°ì‚¬ ë³¸ë¬¸ ì—†ìŒ\")\n",
    "        continue\n",
    "    \n",
    "    keywords = extract_keywords([text], top_n=3)\n",
    "\n",
    "    print(\"ğŸ“Œ í‚¤ì›Œë“œ:\")\n",
    "    for word, score in keywords:\n",
    "        print(f\"- {word}: {round(score, 4)}\")\n",
    "\n",
    "\n",
    "   # extract_and_explain(url, db_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
